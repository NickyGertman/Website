__author__ = 'nicky'

# inspired by: http://scikit-learn.org/dev/auto_examples/applications/face_recognition.html
# and http://iamtrask.github.io/2015/07/12/basic-python-network/


import numpy as np
from sklearn import cross_validation
from sklearn import decomposition
from sklearn import svm
from PIL import Image


imageArray = np.array([])
isWaldoArray = np.array([])

#Build up the training list
#Go through the sample images and assign them their x values (pixel array values) and their Y values (0 or 1)
#The images are already all cropped to be 50x50 px
for i in range (1,26):
    img = Image.open('TrainingImages/Waldo/waldo' + i.__str__() + '.jpg')
    img = img.convert('L')
    #in order to use the image as numerical data, we need to convert it to a numpy array
    imageArray = np.append(imageArray, np.asarray(img, dtype=np.uint8)) #convert the image to a numpy array of pixels
    isWaldoArray = np.append(isWaldoArray,1) #Waldo's faces get assigned a true valuation
    img = Image.open('TrainingImages/Not_Waldo/notWaldo' + i.__str__()+ '.jpg')
    img = img.convert('L')
    #in order to use the image as numerical data, we need to convert it to a numpy array
    imageArray = np.append(imageArray, np.asarray(img, dtype=np.uint8)) #convert the image to a numpy array of pixels
    isWaldoArray = np.append(isWaldoArray,0) #non Waldo faces get assigned a false valuation

#change the shape of the array so that both input and output arrays have the same shape
imageArray = np.reshape(imageArray, (isWaldoArray.shape[0], -1))

print 'Calculating K Fold Cross Validation...'
kFold = cross_validation.StratifiedKFold(isWaldoArray, 10)    #splits the data up into training and testing sets. I did this to try to make the training data more randomized in the hopes of making the training better.
for train_index, test_index in kFold:
        X_train, X_test = imageArray[train_index], imageArray[test_index]
        Y_train = isWaldoArray[train_index]
        Y_test = isWaldoArray[test_index]


#reduce dimensions of the data

pca = decomposition.RandomizedPCA(whiten=True)  #"dimensionality reduction using approximated Singular Value Decomposition of the data and keeping only the most significant singular vectors" - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.RandomizedPCA.html
pca.fit(X_train)    #find the link between the input and output data.
X_train_pca = pca.transform(X_train)    #apply the dimension reduction to training data
X_test_pca = pca.transform(X_test)      #apply the dimension reduction to testing data

# training classification
trainingClassifier = svm.SVC(C=5., gamma=0.00001)   #Support Vector Machine Classification
trainingClassifier.fit(X_train_pca, Y_train)        #fit the data to try to make the classification as tight as possible
res = trainingClassifier.predict(X_test_pca)        #fit the data to try to make the classification as tight as possible
correctPredictions = 0


totalPredictions = 4.0
correctPredictions = 0

#go through the results and increase the count of the correct predictions whenever a prediction was correct.
for i in range(0, len(res)):
    if res[i] == Y_test[i]:
        correctPredictions += 1

score = correctPredictions/totalPredictions * 100

print '\tAccuracy of Predictions: ' + str(score) + '%'
print 'Results' + str(res)
print 'Expected' + str(Y_test)





